"""
main_analyzer.py
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å Yandex GPT
"""

import pandas as pd
import json
import re
import glob
import os
from datetime import datetime, timedelta
import requests
from transformers import pipeline
import spacy
import yfinance as yf
from collections import Counter

# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø YANDEX GPT ==========
YANDEX_API_KEY = ""
YANDEX_FOLDER_ID = ""
YANDEX_BASE_URL = "https://llm.api.cloud.yandex.net/v1/text/completion"

# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ==========
SAFE_MODE = True  # –í–∫–ª—é—á–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º (—Å–∫—Ä—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏)

# ========== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï ==========
# –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ
tickers_raw_main = [
    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ —Ç–∏–∫–µ—Ä—ã
    "GAZP", "SBER", "ROSN", "LKOH", "NVTK", "YNDX", "VTBR", "GMKN", "PLZL",
    "MTSS", "TATN", "ALRS", "POLY", "MGNT", "AFKS", "PHOR", "RUAL", "CHMF",
    "MOEX", "TCSG", "QIWI", "OZON", "DSKY", "LSRG", "FEES", "RTKM", "HYDR",
    
    # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã
    "AAPL", "TSLA", "MSFT", "GOOGL", "AMZN", "META", "NVDA", "INTC", "AMD",
    "NFLX", "ADBE", "CRM", "PYPL", "IBM", "ORCL", "CSCO", "AVGO", "QCOM",
    
    # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞
    "BTC", "ETH", "XRP", "ADA", "SOL", "DOT", "DOGE", "SHIB",
    
    # –í–∞–ª—é—Ç–∞ –∏ —Å—ã—Ä—å–µ
    "XAU", "XAG", "XPT", "XPD",
    "BRENT", "WTI",
    "EURUSD", "USDJPY", "GBPUSD", "USDCHF", "AUDUSD", "USDCAD", "NZDUSD",
    "RUB", "USD", "EUR", "CNY", "JPY", "GBP"
]

countries = ["–í–µ–Ω–µ—Å—É—ç–ª–∞", "–°–®–ê", "–†–æ—Å—Å–∏—è", "–ö–∏—Ç–∞–π", "–§—Ä–∞–Ω—Ü–∏—è", "–ì–µ—Ä–º–∞–Ω–∏—è", 
             "–°–∞—É–¥–æ–≤—Å–∫–∞—è –ê—Ä–∞–≤–∏—è", "–ò–Ω–¥–∏—è", "–£–∫—Ä–∞–∏–Ω–∞", "–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è", 
             "–Ø–ø–æ–Ω–∏—è", "–Æ–∂–Ω–∞—è –ö–æ—Ä–µ—è", "–¢—É—Ä—Ü–∏—è", "–û–ê–≠", "–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω"]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
sentiment_analyzer = None
nlp = None
yandex_analyzer = None

# ========== –§–£–ù–ö–¶–ò–ò ==========
def clean_channels(df):
    """–°–∫—Ä—ã–≤–∞–µ—Ç –∏–º–µ–Ω–∞ –∫–∞–Ω–∞–ª–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
    if SAFE_MODE and 'channel' in df.columns:
        channel_mapping = {
            'economica': 'analytics_source_1',
            'headlines_for_traders': 'analytics_source_2',
            'headlines_macro': 'analytics_source_3',
            'headlines_quants': 'analytics_source_4',
            'alfa_investments': 'analytics_source_5',
            'tb_invest_official': 'analytics_source_6'
        }
        df['channel'] = df['channel'].map(channel_mapping).fillna('financial_source')
    return df

def get_moex_price(ticker, date):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ü–µ–Ω—É –∞–∫—Ü–∏–∏ —Å MOEX ISS"""
    try:
        date_str = date.strftime("%Y-%m-%d")
        url = f"https://iss.moex.com/iss/history/engines/stock/markets/shares/securities/{ticker}.json"
        params = {
            'from': date_str,
            'till': date_str,
            'start': 0
        }
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        history = data.get('history', {})
        columns = history.get('columns', [])
        data_rows = history.get('data', [])
        
        if not data_rows:
            print(f"‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ {date_str}")
            return None
        
        if 'CLOSE' in columns:
            close_idx = columns.index('CLOSE')
            close_price = float(data_rows[0][close_idx])
            return close_price
        else:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü CLOSE –¥–ª—è {ticker}")
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {ticker}: {e}")
        return None
    except (json.JSONDecodeError, KeyError, IndexError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {ticker}: {e}")
        return None

def load_all_posts(base_path="data"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –ø–æ—Å—Ç—ã –∏–∑ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–æ–∫"""
    all_posts = []
    pattern = os.path.join(base_path, "*", "posts.json")
    json_files = glob.glob(pattern)
    
    if not json_files:
        alt_pattern = os.path.join(base_path, "posts.json")
        if os.path.exists(alt_pattern):
            json_files = [alt_pattern]
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(json_files)} —Ñ–∞–π–ª–æ–≤ posts.json")
    
    for file_path in json_files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                posts = json.load(f)
                all_posts.extend(posts)
                print(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –∏–∑ {os.path.basename(os.path.dirname(file_path))}")
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
    
    return all_posts

def analyze_sentiment(text):
    """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
    global sentiment_analyzer
    if not isinstance(text, str) or len(text.strip()) == 0:
        return 0.0
    try:
        if sentiment_analyzer is None:
            return 0.0
        result = sentiment_analyzer(text[:512])[0]
        if result["label"] == "POSITIVE":
            return result["score"]
        elif result["label"] == "NEGATIVE":
            return -result["score"]
        else:
            return 0.0
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return 0.0

def find_mentions(text):
    """–ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–∏–∫–µ—Ä–æ–≤ –∏ —Å—Ç—Ä–∞–Ω –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not isinstance(text, str) or pd.isna(text):
        return []
    
    found = []
    text_upper = text.upper()

    # –ò—â–µ–º —Ç–∏–∫–µ—Ä—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
    for ticker in tickers_raw_main:
        if re.search(r'\b' + re.escape(ticker) + r'\b', text_upper):
            found.append(ticker)

    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
    russian_companies = {
        "–•—ç–¥—Ö–∞–Ω—Ç–µ—Ä": "HH",
        "–ú–î –ú–µ–¥–∏–∫–∞–ª –ì—Ä—É–ø–ø": "MDMG",
        "–ò–Ω—Ç–µ—Ä –†–ê–û": "IRAO",
        "–ü–æ–ª—é—Å": "PLZL",
        "–Ø–Ω–¥–µ–∫—Å": "YNDX",
        "–ò–ö–° 5": "?",
        "–¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "TCSG",
        "–°–±–µ—Ä": "SBER",
        "–ì–∞–∑–ø—Ä–æ–º": "GAZP",
        "–õ—É–∫–æ–π–ª": "LKOH",
        "–†–æ—Å–Ω–µ—Ñ—Ç—å": "ROSN"
    }

    for name, ticker in russian_companies.items():
        if name in text:
            found.append(ticker)

    # –°—Ç—Ä–∞–Ω—ã
    for country in countries:
        if re.search(r'\b' + re.escape(country.lower()) + r'\b', text.lower()):
            found.append(f"COUNTRY_{country}")

    return list(set(found))

def enhanced_analysis(text, tickers):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏"""
    if not isinstance(text, str):
        return {}

    text_lower = text.lower()
    analysis = {
        "—Ç–µ–º—ã": [],
        "—Å–µ–∫—Ç–æ—Ä–∞": [],
        "—Ç–∏–∫–µ—Ä—ã": tickers,
        "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è_—Ä–µ–∞–∫—Ü–∏—è": "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
    }

    themes_keywords = {
        "–±—é–¥–∂–µ—Ç": ["–±—é–¥–∂–µ—Ç", "–¥–µ—Ñ–∏—Ü–∏—Ç", "–ø—Ä–æ—Ñ–∏—Ü–∏—Ç", "—Ñ–∏–Ω–∞–Ω—Å—ã", "–∫–∞–∑–Ω", "—Ç—Ä–ª–Ω", "–º–ª—Ä–¥", "—Ä–∞—Å—Ö–æ–¥—ã", "–¥–æ—Ö–æ–¥—ã"],
        "–Ω–µ—Ñ—Ç—å –∏ –≥–∞–∑": ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "–Ω–µ—Ñ—Ç—è–Ω–æ–π", "–≥–∞–∑–æ–≤—ã–π", "–Ω–µ—Ñ—Ç–µ–≥–∞–∑", "brent", "wti", "–Ω–µ—Ñ—Ç—è–Ω–∏–∫–∏"],
        "–±–∞–Ω–∫–∏": ["–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "—Å—Ç–∞–≤–∫–∞", "–∏–ø–æ—Ç–µ–∫", "–≤–∫–ª–∞–¥", "—Ü–±", "—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫", "—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä"],
        "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏", "it", "—Å–æ—Ñ—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∏–∏", "ai", "—Ü–∏—Ñ—Ä–æ–≤"],
        "–º–µ—Ç–∞–ª–ª—ã": ["–∑–æ–ª–æ—Ç", "—Å–µ—Ä–µ–±—Ä", "–º–µ—Ç–∞–ª–ª", "–º–µ–¥—å", "–∞–ª—é–º–∏–Ω", "–Ω–∏–∫–µ–ª—å", "–ø–ª–∞—Ç–∏–Ω", "–ø–∞–ª–ª–∞–¥–∏–π"],
        "–≤–∞–ª—é—Ç–∞": ["—Ä—É–±–ª", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "—é–∞–Ω", "–π–µ–Ω", "—Ñ—Ä–∞–Ω–∫", "—Ñ—É–Ω—Ç", "–∫—É—Ä—Å", "–æ–±–º–µ–Ω", "–≤–∞–ª—é—Ç"],
        "—Å–∞–Ω–∫—Ü–∏–∏": ["—Å–∞–Ω–∫—Ü", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω", "–∑–∞–ø—Ä–µ—Ç", "—ç–º–±–∞—Ä–≥–æ", "–±–ª–æ–∫–∏—Ä–æ–≤–∫", "–∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω"],
        "–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞": ["–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä", "—Å—Ç—Ä–æ–π–∫", "–¥–æ—Ä–æ–≥", "–º–æ—Å—Ç", "—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥", "–º–∞–≥–∏—Å—Ç—Ä–∞–ª"],
        "—Å–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ": ["—Å–µ–ª—å—Å–∫", "–∞–≥—Ä–æ", "–∑–µ—Ä–Ω", "–ø—à–µ–Ω–∏—Ü", "–∫—É–∫—É—Ä—É–∑", "—É—Ä–æ–∂–∞–π", "—Ñ–µ—Ä–º", "–ø–æ—Å–µ–≤"],
    }

    # –ü–æ–∏—Å–∫ —Ç–µ–º
    for theme, keywords in themes_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                if theme not in analysis["—Ç–µ–º—ã"]:
                    analysis["—Ç–µ–º—ã"].append(theme)
                break

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–æ–≤
    sector_mapping = {
        "–±—é–¥–∂–µ—Ç": ["–±–∞–Ω–∫–∏", "–≥–æ—Å–∑–∞–∫–∞–∑", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "–Ω–µ—Ñ—Ç—å –∏ –≥–∞–∑": ["—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "—Å—ã—Ä—å–µ", "–¥–æ–±—ã—á–∞"],
        "–±–∞–Ω–∫–∏": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏"],
        "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": ["it", "—Ç–µ–ª–µ–∫–æ–º", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"],
        "–º–µ—Ç–∞–ª–ª—ã": ["–¥–æ–±—ã—á–∞", "—Å—ã—Ä—å–µ", "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å"],
        "–≤–∞–ª—é—Ç–∞": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "—Ç–æ—Ä–≥–æ–≤–ª—è", "—ç–∫—Å–ø–æ—Ä—Ç/–∏–º–ø–æ—Ä—Ç"],
        "—Å–∞–Ω–∫—Ü–∏–∏": ["–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è", "—Ç–æ—Ä–≥–æ–≤–ª—è", "–ª–æ–≥–∏—Å—Ç–∏–∫–∞"],
        "–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞": ["—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã", "–ª–æ–≥–∏—Å—Ç–∏–∫–∞", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
        "—Å–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ": ["–∞–≥—Ä–æ—Å–µ–∫—Ç–æ—Ä", "–ø–∏—â–µ–≤–∞—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å", "–ª–æ–≥–∏—Å—Ç–∏–∫–∞"]
    }

    for theme in analysis["—Ç–µ–º—ã"]:
        if theme in sector_mapping:
            for sector in sector_mapping[theme]:
                if sector not in analysis["—Å–µ–∫—Ç–æ—Ä–∞"]:
                    analysis["—Å–µ–∫—Ç–æ—Ä–∞"].append(sector)

    ticker_sectors = {
        "SBER": ["–±–∞–Ω–∫–∏", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "VTBR": ["–±–∞–Ω–∫–∏", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "GAZP": ["—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "–≥–∞–∑", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã"],
        "ROSN": ["–Ω–µ—Ñ—Ç—å", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã"],
        "LKOH": ["–Ω–µ—Ñ—Ç—å", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
        "NVTK": ["–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
        "AAPL": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "it", "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã"],
        "TSLA": ["–∞–≤—Ç–æ–º–æ–±–∏–ª–∏", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∑–µ–ª–µ–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è"],
        "MSFT": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "it", "–æ–±–ª–∞—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"],
        "GOOGL": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Ä–µ–∫–ª–∞–º–∞", "it"],
        "AMZN": ["—Ä–∏—Ç–µ–π–ª", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–ª–æ–≥–∏—Å—Ç–∏–∫–∞"],
        "BTC": ["–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏"],
        "ETH": ["–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
    }

    for ticker in tickers:
        # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ
        ticker_clean = ticker.split('-')[0] if '-' in ticker else ticker
        ticker_clean = ticker_clean.split('.')[0] if '.' in ticker_clean else ticker_clean
        if ticker_clean in ticker_sectors:
            for sector in ticker_sectors[ticker_clean]:
                if sector not in analysis["—Å–µ–∫—Ç–æ—Ä–∞"]:
                    analysis["—Å–µ–∫—Ç–æ—Ä–∞"].append(sector)

    # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–∫—Ü–∏—è
    historical_reactions = {
        "SBER": "–ø—Ä–∏ –Ω–æ–≤–æ—Å—Ç—è—Ö –æ –¥–µ—Ñ–∏—Ü–∏—Ç–µ –±—é–¥–∂–µ—Ç–∞ SBER –ø–∞–¥–∞–ª 2-3%",
        "GAZP": "–Ω–æ–≤–æ—Å—Ç–∏ –æ –±—é–¥–∂–µ—Ç–µ —Å–ª–∞–±–æ –≤–ª–∏—è—é—Ç –Ω–∞ GAZP (0-1%)",
        "VTBR": "—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –±—é–¥–∂–µ—Ç–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å 3-5%",
        "AAPL": "—Å–ª–∞–±–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –±—é–¥–∂–µ—Ç–æ–º",
        "TSLA": "–Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –±—é–¥–∂–µ—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π",
        "ROSN": "–ø—Ä–∏ —Ä–æ—Å—Ç–µ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å ROSN —Ä–∞—Å—Ç–µ—Ç 5-7%",
        "LKOH": "—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –Ω–µ—Ñ—Ç—è–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å 4-6%",
        "BTC": "—Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å 5-10%"
    }

    reactions = []
    for ticker in tickers:
        ticker_clean = ticker.split('-')[0] if '-' in ticker else ticker
        ticker_clean = ticker_clean.split('.')[0] if '.' in ticker_clean else ticker_clean
        if ticker_clean in historical_reactions:
            reactions.append(historical_reactions[ticker_clean])

    if reactions:
        analysis["–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è_—Ä–µ–∞–∫—Ü–∏—è"] = " | ".join(reactions[:3])

    return analysis

def enhanced_sentiment_analysis(text):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    if not isinstance(text, str):
        return 0.0

    # 1. –°–Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å
    base_sentiment = 0.0
    if sentiment_analyzer is not None:
        try:
            result = sentiment_analyzer(text[:512])[0]
            base_sentiment = result["score"] if result["label"] == "POSITIVE" else -result["score"]
        except:
            base_sentiment = 0.0

    # 2. –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    text_lower = text.lower()

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –†–û–°–¢–ê
    growth_words = [
        "—Ä–æ—Å—Ç", "–≤—ã—à–µ", "—É–≤–µ–ª–∏—á", "–ø—Ä–∏–±—ã–ª—å", "—Ä–µ–∫–æ—Ä–¥", "—É—Å–∏–ª–∏", "+", "–ø–æ–≤—ã—Å–∏",
        "–ø—Ä–æ–≥—Ä–µ—Å—Å", "—É–ª—É—á—à", "–ø–æ–±–µ–¥–∞", "–¥–æ—Å—Ç–∏–∂–µ–Ω", "–ø–æ–∑–∏—Ç–∏–≤", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
        "–ø—Ä–∏–≤–ª–µ–∫", "—É—Å–ø–µ—Ö", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—É–≤–µ—Ä–µ–Ω"
    ]

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ü–ê–î–ï–ù–ò–Ø  
    decline_words = [
        "–ø–∞–¥–µ–Ω–∏–µ", "–Ω–∏–∂–µ", "—Å–Ω–∏–∂–µ–Ω", "—É–±—ã—Ç–æ–∫", "–∫—Ä–∏–∑–∏—Å", "–æ—Å–ª–∞–±–ª", "-", "–ø–æ–Ω–∏–∑–∏",
        "–ø—Ä–æ–±–ª–µ–º", "—Ä–∏—Å–∫", "–Ω–µ–≥–∞—Ç–∏–≤", "–ø—Ä–æ–∏–≥—Ä—ã—à", "–ø–æ—Ç–µ—Ä—è", "—Å–ø–∞–¥", "–æ–±–≤–∞–ª",
        "—Å–æ–∫—Ä–∞—â", "—É–≤–æ–ª—å–Ω", "—Å–∞–Ω–∫—Ü–∏–∏", "–∑–∞–ø—Ä–µ—Ç", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω"
    ]

    # –°—á–∏—Ç–∞–µ–º –≤–µ—Å
    growth_count = sum(1 for word in growth_words if word in text_lower)
    decline_count = sum(1 for word in decline_words if word in text_lower)

    # 3. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if decline_count > growth_count:
        # –ï—Å—Ç—å —è–≤–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ - –¥–µ–ª–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω–µ–µ
        return min(base_sentiment, -0.3)
    elif growth_count > decline_count:
        # –ï—Å—Ç—å —è–≤–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ - –¥–µ–ª–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω–µ–µ
        return max(base_sentiment, 0.3)
    else:
        # –ù–µ—Ç —è–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–¥–µ–ª–∏
        return base_sentiment

def generate_safe_title(text):
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–µ–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    if not isinstance(text, str):
        return "–ù–æ–≤–æ—Å—Ç—å –æ —Ä—ã–Ω–∫–µ"

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏, —Ö—ç—à—Ç–µ–≥–∏, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    import re
    clean_text = re.sub(r'@\w+|#\w+|http\S+', '', text)

    # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keywords = []
    text_lower = clean_text.lower()

    market_words = ["–∞–∫—Ü–∏–∏", "–Ω–µ—Ñ—å", "–≥–∞–∑", "—Ä—É–±–ª—å", "–¥–æ–ª–ª–∞—Ä", "–∏–Ω–¥–µ–∫—Å", "—Ä—ã–Ω–æ–∫", "–±–∏—Ä–∂–∞", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏"]
    country_words = ["–†–æ—Å—Å–∏—è", "–°–®–ê", "–ö–∏—Ç–∞–π", "–ï–≤—Ä–æ–ø–∞", "–í–µ–Ω–µ—Å—É—ç–ª–∞", "–°–∞—É–¥–æ–≤—Å–∫–∞—è", "–ö–æ—Ä–µ—è"]

    for word in market_words:
        if word in text_lower:
            keywords.append(word)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –Ω–æ–≤–æ—Å—Ç–∏
    if any(word in text_lower for word in ["—Ä–æ—Å—Ç", "–≤—ã—Ä–æ—Å", "–ø—Ä–∏–±–∞–≤–∏–ª", "+", "—É–≤–µ–ª–∏—á–∏–ª"]):
        action = "—Ä–æ—Å—Ç"
    elif any(word in text_lower for word in ["–ø–∞–¥–µ–Ω–∏–µ", "—Å–Ω–∏–∑–∏–ª—Å—è", "—É–ø–∞–ª", "-", "–æ–±–≤–∞–ª"]):
        action = "–ø–∞–¥–µ–Ω–∏–µ"
    else:
        action = "–Ω–æ–≤–æ—Å—Ç–∏"

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if keywords:
        return f"{action.capitalize()} –Ω–∞ —Ä—ã–Ω–∫–µ {keywords[0]}"
    else:
        return f"–†—ã–Ω–æ—á–Ω—ã–µ {action}"

def get_main_topic(text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É –Ω–æ–≤–æ—Å—Ç–∏"""
    if not isinstance(text, str):
        return "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏"

    text_lower = text.lower()

    if any(word in text_lower for word in ["–∏–Ω–¥–µ–∫—Å", "–º–æ—Å–±–∏—Ä–∂", "–∞–∫—Ü–∏–∏", "—Å–±–µ—Ä", "–≥–∞–∑–ø—Ä–æ–º", "—è–Ω–¥–µ–∫—Å"]):
        return "–§–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫ –†–æ—Å—Å–∏–∏"
    elif any(word in text_lower for word in ["—Å–µ—Ä–µ–±—Ä–æ", "–∑–æ–ª–æ—Ç–æ", "xag", "xau", "–º–µ—Ç–∞–ª–ª—ã"]):
        return "–î—Ä–∞–≥–æ—Ü–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–ª–ª—ã"
    elif any(word in text_lower for word in ["–Ω–µ—Ñ—Ç—å", "brent", "–≥–∞–∑", "—Ä–æ—Å–∏–Ω", "–ª—É–∫–æ–π–ª"]):
        return "–°—ã—Ä—å–µ–≤—ã–µ —Ä—ã–Ω–∫–∏"
    elif any(word in text_lower for word in ["–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏"]):
        return "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"
    elif any(word in text_lower for word in ["–≤–µ–Ω–µ—Å—É—ç–ª–∞", "—Å–∞–Ω–∫—Ü–∏–∏", "–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞", "—Å—à–∞", "—Ä–æ—Å—Å–∏—è"]):
        return "–ì–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞"
    elif any(word in text_lower for word in ["–∏—Ç–æ–≥–∏ –≥–æ–¥–∞", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]):
        return "–†—ã–Ω–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
    else:
        return "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏"

def get_market_impact(text, sentiment):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—ã–Ω–æ–∫"""
    # –°—á–∏—Ç–∞–µ–º "–≤–µ—Å" –Ω–æ–≤–æ—Å—Ç–∏
    text_len = len(str(text))
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
    high_impact_words = ["–∫—Ä–∏–∑–∏—Å", "–æ–±–≤–∞–ª", "–≤–æ–π–Ω–∞", "—Å–∞–Ω–∫—Ü–∏–∏", "–¥–µ—Ñ–æ–ª—Ç", "–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ"]
    medium_impact_words = ["—Ä–æ—Å—Ç", "–ø–∞–¥–µ–Ω–∏–µ", "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "–æ—Ç—á–µ—Ç", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"]
    
    impact_score = 0
    text_lower = str(text).lower()
    
    for word in high_impact_words:
        if word in text_lower:
            impact_score += 3
    
    for word in medium_impact_words:
        if word in text_lower:
            impact_score += 1

    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    if abs(sentiment) > 0.5:
        impact_score += 2
    elif abs(sentiment) > 0.3:
        impact_score += 1

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å
    if impact_score >= 4:
        return "–í—ã—Å–æ–∫–æ–µ"
    elif impact_score >= 2:
        return "–°—Ä–µ–¥–Ω–µ–µ"
    else:
        return "–ù–∏–∑–∫–æ–µ"

def get_top_tickers(limit=10):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º –¥–ª—è –±–æ—Ç–∞"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists('detailed_posts.json'):
            with open('detailed_posts.json', 'r', encoding='utf-8') as f:
                posts = json.load(f)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            return get_top_tickers_from_df(limit)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–∏–∫–µ—Ä—ã
        all_tickers = []
        for post in posts:
            if 'tickers' in post:
                all_tickers.extend(post['tickers'])
        
        if not all_tickers:
            return get_top_tickers_from_df(limit)
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        from collections import Counter
        ticker_counts = Counter(all_tickers)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-N
        return ticker_counts.most_common(limit)
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ get_top_tickers: {e}")
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
        return [("GAZP", 15), ("SBER", 12), ("ROSN", 8), ("AAPL", 6), ("TSLA", 5)]

def get_top_tickers_from_df(df, limit=10):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø —Ç–∏–∫–µ—Ä–æ–≤ –∏–∑ DataFrame"""
    if df.empty:
        return []
    
    all_tickers = []
    for mentions in df['mentions']:
        if isinstance(mentions, list):
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–∏–∫–µ—Ä—ã (–Ω–µ —Å—Ç—Ä–∞–Ω—ã)
            tickers = [m for m in mentions if not m.startswith('COUNTRY_')]
            all_tickers.extend(tickers)
    
    ticker_counts = Counter(all_tickers)
    return ticker_counts.most_common(limit)

def init_yandex_analyzer():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä"""
    global yandex_analyzer
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª—è Yandex GPT
        from yandex_analyzer import YandexGPTAnalyzer, init_analyzer
        
        print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
        success = init_analyzer(YANDEX_API_KEY, YANDEX_FOLDER_ID, YANDEX_BASE_URL)
        if success:
            print("‚úÖ Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
            return True
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä!")
            return False
    except ImportError as e:
        print(f"‚ö†Ô∏è  –ú–æ–¥—É–ª—å yandex_analyzer –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Yandex GPT: {e}")
        return False

def generate_daily_report():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç —Å AI-–∞–Ω–∞–ª–∏–∑–æ–º"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
    global yandex_analyzer
    if yandex_analyzer is None:
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
        init_yandex_analyzer()
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        posts = load_all_posts("data")
        df = pd.DataFrame(posts)
        
        if df.empty:
            return "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        
        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        df["mentions"] = df["text"].apply(find_mentions)
        df = df[df["mentions"].apply(len) > 0]  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç—ã —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
        df["sentiment"] = df['text'].apply(enhanced_sentiment_analysis)
        df["topic"] = df['text'].apply(get_main_topic)
        df["impact"] = df.apply(lambda row: get_market_impact(row['text'], row['sentiment']), axis=1)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–Ω–æ–≤–æ—Å—Ç–∏ —Å —Ç–∏–∫–µ—Ä–∞–º–∏ + –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ = –≤–∞–∂–Ω—ã–µ)
        df["importance_score"] = df["mentions"].apply(len) * 10 + abs(df["sentiment"]) * 5
        df = df.sort_values("importance_score", ascending=False)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞
        top_news = df.head(3).to_dict('records')
        
        report_lines = []
        report_lines.append("üöÄ **AI‚Äë–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –î–ê–ô–î–ñ–ï–°–¢**")
        report_lines.append(f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}")
        report_lines.append("=" * 50)
        
        ai_used = False
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Ç–æ–ø‚Äë–Ω–æ–≤–æ—Å—Ç—å
        for i, news in enumerate(top_news, 1):
            text = news.get('text', '')
            mentions = news.get('mentions', [])
            tickers = [m for m in mentions if not m.startswith('COUNTRY_')]
            sentiment = news.get('sentiment', 0.0)
            topic = news.get('topic', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            impact = news.get('impact', '–ù–∏–∑–∫–æ–µ')
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–≤—å—é –Ω–æ–≤–æ—Å—Ç–∏
            preview = text[:120] + "..." if len(text) > 120 else text
            
            report_lines.append(f"\nüì∞ **–ù–û–í–û–°–¢–¨ {i}**")
            report_lines.append(f"üìå –¢–µ–º–∞: {topic}")
            report_lines.append(f"üéØ –í–ª–∏—è–Ω–∏–µ: {impact}")
            report_lines.append(f"üìä –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment:.2f}")
            report_lines.append(f"üîç –£–ø–æ–º–∏–Ω–∞–Ω–∏—è: {', '.join(tickers) if tickers else '–Ω–µ—Ç'}")
            report_lines.append(f"üí¨ –ü—Ä–µ–≤—å—é: {preview}")
            
            # AI‚Äë–∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Yandex GPT (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            try:
                if yandex_analyzer:
                    ai_prompt = (
                        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ –∫—Ä–∞—Ç–∫–æ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:\n"
                        f"1. –°—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ (1‚Äë2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).\n"
                        f"2. –ö–∞–∫–∏–µ –∞–∫—Ç–∏–≤—ã/—Ä—ã–Ω–∫–∏ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã?\n"
                        f"3. –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ).\n"
                        f"4. –†–∏—Å–∫–∏ (1‚Äë2 –∫–ª—é—á–µ–≤—ã—Ö —Ä–∏—Å–∫–∞).\n\n"
                        f"–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏: {text}"
                    )
                    ai_response = yandex_analyzer.generate_text(ai_prompt, temperature=0.7, max_tokens=200)
                    if ai_response:
                        ai_used = True
                        report_lines.append(f"\nü§ñ **AI‚Äë–∞–Ω–∞–ª–∏–∑**:\n{ai_response}")
                    else:
                        report_lines.append("\nü§ñ **AI‚Äë–∞–Ω–∞–ª–∏–∑**: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
            except Exception as e:
                report_lines.append(f"\n‚ö†Ô∏è **AI‚Äë–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω**: {str(e)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        report_lines.append("\n" + "=" * 50)
        report_lines.append("üìä **–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ù–Ø**")

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        total_news = len(df)
        report_lines.append(f"üîπ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total_news}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º
        if 'topic' in df.columns:
            topic_counts = df['topic'].value_counts()
            report_lines.append("üîπ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º:")
            for topic_name, count in topic_counts.items():
                report_lines.append(f"   - {topic_name}: {count}")

        # –¢–æ–ø-5 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º
        top_tickers = get_top_tickers_from_df(df, 5)
        report_lines.append("üîπ –¢–æ–ø-5 —É–ø–æ–º–∏–Ω–∞–µ–º—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤:")
        for ticker, count in top_tickers:
            report_lines.append(f"   - {ticker}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        # –°—Ä–µ–¥–Ω—è—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if 'sentiment' in df.columns:
            avg_sentiment = df['sentiment'].mean()
            sentiment_label = "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è" if avg_sentiment > 0 else "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è" if avg_sentiment < 0 else "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è"
            report_lines.append(f"üîπ –°—Ä–µ–¥–Ω—è—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–µ–π: {sentiment_label} ({avg_sentiment:.2f})")

        # –£—Ä–æ–≤–µ–Ω—å –≤–ª–∏—è–Ω–∏—è
        if 'impact' in df.columns:
            impact_counts = df['impact'].value_counts()
            report_lines.append("üîπ –£—Ä–æ–≤–µ–Ω—å –≤–ª–∏—è–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π:")
            for impact_name, count in impact_counts.items():
                report_lines.append(f"   - {impact_name}: {count}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ AI
        if ai_used:
            report_lines.append("\nü§ñ –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –ø–æ–º–æ—â—å—é Yandex GPT")
        else:
            report_lines.append("\n‚ö†Ô∏è AI‚Äë–∞–Ω–∞–ª–∏–∑ –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω (–º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        full_report = "\n".join(report_lines)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        report_filename = f"daily_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(full_report)

        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ —Ñ–∞–π–ª: {report_filename}")
        return full_report

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞: {str(e)}"
        print(error_msg)
        return error_msg

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
    global sentiment_analyzer, nlp
    
    print("\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    try:
        sentiment_analyzer = pipeline("sentiment-analysis", model="blanchefort/rubert-base-cased-sentiment")
        print("  ‚úì –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω")
    except Exception as e:
        print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        sentiment_analyzer = None
    
    # spaCy –º–æ–¥–µ–ª—å
    try:
        nlp = spacy.load("ru_core_news_sm")
        print("  ‚úì spaCy –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ spaCy: {e}")
        nlp = None
    
    # Yandex GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    init_yandex_analyzer()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists("data"):
        print("‚ùå –ü–∞–ø–∫–∞ 'data' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–π—Ç–µ –µ—ë –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ —Ñ–∞–π–ª—ã posts.json")
        return
    
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
        report = generate_daily_report()
        
        # –í—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–µ–≤—å—é)
        print("\n" + "="*50)
        print("–ü–†–ï–í–¨–Æ –û–¢–ß–Å–¢–ê:")
        if len(report) > 1000:
            print(report[:1000] + "...")
        else:
            print(report)
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()