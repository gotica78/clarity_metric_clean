"""
yandex_analyzer.py
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Yandex¬†GPT —á–µ—Ä–µ–∑ Yandex¬†Cloud API
"""

import requests
import json
import re
import logging
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import pickle
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
    summary: str
    sentiment: str
    risk_level: str
    affected_assets: List[str]
    market_impact: str
    short_term_action: str
    confidence: float
    analyzed_at: datetime
    is_fallback: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            "summary": self.summary,
            "sentiment": self.sentiment,
            "risk_level": self.risk_level,
            "affected_assets": self.affected_assets,
            "market_impact": self.market_impact,
            "short_term_action": self.short_term_action,
            "confidence": self.confidence,
            "analyzed_at": self.analyzed_at.isoformat(),
            "is_fallback": self.is_fallback
        }

class YandexGPTAnalyzer:
    def __init__(self, api_key: str, folder_id: str, cloud_id: str,
                 base_url: str = "https://llm.api.cloud.yandex.ru"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ Yandex¬†GPT
        
        Args:
            api_key: 
            folder_id: 
            base_url: https://llm.api.cloud.yandex.ru
            cloud_id = 
        """
        self.api_key = api_key
        self.folder_id = folder_id
        self.cloud_id = cloud_id
        self.base_url = base_url.rstrip('/')
        self.completion_url = f"{self.base_url}/foundationModels/v1/completion"
        self.cache = {}
        self.request_count = 0
        self.cache_enabled = True
        self.cache_dir = "cache"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫–µ—à–∞
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–µ—à —Å –¥–∏—Å–∫–∞
        self._load_cache()
        
        logger.info(f"ü§ñ Yandex¬†GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"   Folder ID: {folder_id}")
        logger.info(f"   –ë–∞–∑–æ–≤—ã–π URL: {base_url}")
    
    def _get_cache_key(self, text: str, analysis_type: str = "news") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è –∫–µ—à–∞"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        return f"{analysis_type}_{text_hash}"
    
    def _load_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–µ—à–∞ —Å –¥–∏—Å–∫–∞"""
        cache_file = os.path.join(self.cache_dir, "yandex_gpt_cache.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    self.cache = pickle.load(f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∫–µ—à –∏–∑ {cache_file}, –∑–∞–ø–∏—Å–µ–π: {len(self.cache)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
    
    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–µ—à–∞ –Ω–∞ –¥–∏—Å–∫"""
        if not self.cache_enabled:
            return
        
        cache_file = os.path.join(self.cache_dir, "yandex_gpt_cache.pkl")
        try:
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (—Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π)
            week_ago = datetime.now() - timedelta(days=7)
            for key in list(self.cache.keys()):
                if self.cache[key]['timestamp'] < week_ago:
                    del self.cache[key]
            
            with open(cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
            logger.debug(f"–ö–µ—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {cache_file}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")
    
    def analyze_news(self, news_text: str, mentioned_tickers: List[str] = None, 
                    use_cache: bool = True) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Yandex¬†GPT API
        
        Args:
            news_text: –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
            mentioned_tickers: –°–ø–∏—Å–æ–∫ —É–ø–æ–º—è–Ω—É—Ç—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        if mentioned_tickers is None:
            mentioned_tickers = []
        
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ ({len(news_text)} —Å–∏–º–≤–æ–ª–æ–≤): {news_text[:80]}...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–∞
        if not self.api_key:
            logger.warning("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–π API –∫–ª—é—á")
            return self._create_basic_fallback(news_text, mentioned_tickers)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_key = self._get_cache_key(news_text, "news")
        if use_cache and self.cache_enabled and cache_key in self.cache:
            cached = self.cache[cache_key]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞—Ä–µ–ª–∏ –ª–∏ –¥–∞–Ω–Ω—ã–µ (–º–∞–∫—Å–∏–º—É–º 1 –¥–µ–Ω—å)
            if datetime.now() - cached['timestamp'] < timedelta(days=1):
                logger.info("üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                return cached['data']
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            headers = {
                "Authorization": f"Api-Key {self.api_key}",
                "Content-Type": "application/json",
                "x-folder-id": self.folder_id
            }
            
            # –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É)
            text_to_analyze = news_text[:3000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Yandex GPT
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            prompt = f"""–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Å 15-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã –Ω–∞ —Ä—ã–Ω–∫–µ.
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ –¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

            –¢–ï–ö–°–¢ –ù–û–í–û–°–¢–ò:
            {text_to_analyze}

            –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
            ‚Ä¢ –£–ø–æ–º—è–Ω—É—Ç—ã–µ –∞–∫—Ç–∏–≤—ã: {', '.join(mentioned_tickers) if mentioned_tickers else '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}
            ‚Ä¢ –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {datetime.now().strftime('%d.%m.%Y %H:%M')}
            ‚Ä¢ –†—ã–Ω–∫–∏: –†–æ—Å—Å–∏–π—Å–∫–∏–π, –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π, —Å—ã—Ä—å–µ–≤–æ–π

            –¢–†–ï–ë–£–ï–ú–´–ô –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
            {{
                "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–æ–≤–æ—Å—Ç–∏ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
                "detailed_analysis": "–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—Ä–∏—á–∏–Ω –∏ —Å–ª–µ–¥—Å—Ç–≤–∏–π",
                "sentiment": "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
                "sentiment_score": —á–∏—Å–ª–æ –æ—Ç -1.0 –¥–æ 1.0,
                "risk_level": "–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π",
                "risk_explanation": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞",
                "affected_assets": ["—Å–ø–∏—Å–æ–∫", "–∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö", "—Ç–∏–∫–µ—Ä–æ–≤"],
                "market_impact": "–û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ä—ã–Ω–∫–∏ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
                "short_term_forecast": "–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1-3 –¥–Ω—è",
                "recommended_actions": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞",
                "key_risks": ["—Å–ø–∏—Å–æ–∫", "–∫–ª—é—á–µ–≤—ã—Ö", "—Ä–∏—Å–∫–æ–≤"],
                "opportunities": ["—Å–ø–∏—Å–æ–∫", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"],
                "confidence": 0.95
            }}

            –í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:
            1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º
            2. –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏–∏, —É–∫–∞–∑—ã–≤–∞–π –∏—Ö —Ç–∏–∫–µ—Ä—ã
            3. –£—á–∏—Ç—ã–≤–∞–π –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            4. –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            5. –ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑
            """
            
            payload = {
                "modelUrl": f"gpt://{self.cloud_id}/{self.folder_id}/yandexgpt-lite",
                "completionOptions": {
                    "temperature": 0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    "maxTokens": 1500,   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    "stream": False
                },
                "messages": [
                    {
                        "role": "system",
                        "text": "–¢—ã ‚Äî –≥–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫—Ä—É–ø–Ω–æ–≥–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –±–∞–Ω–∫–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ, –∫—Ä–∞—Ç–∫–∏–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Ç—Ä–µ–π–¥–µ—Ä–æ–≤ –∏ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤."
                    },
                    {
                        "role": "user",
                        "text": prompt
                    }
                ]
            }
            
            logger.info("üì° –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Yandex¬†GPT API...")
            self.request_count += 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É, –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            if self.request_count % 5 == 0:
                logger.debug("‚è≥ –ü–∞—É–∑–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API...")
                time.sleep(1)
            
            response = requests.post(
                self.completion_url,
                headers=headers,
                json=payload,
                timeout=30  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
            )
            
            logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: —Å—Ç–∞—Ç—É—Å {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                ai_text = result["result"]["alternatives"][0]["message"]["text"]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                analysis = self._extract_json_from_response(ai_text)
                
                if analysis and "summary" in analysis:
                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    analysis["analyzed_at"] = datetime.now().isoformat()
                    analysis["text_hash"] = hashlib.md5(news_text.encode()).hexdigest()[:8]
                    analysis["is_fallback"] = False
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                    if use_cache and self.cache_enabled:
                        self.cache[cache_key] = {
                            'timestamp': datetime.now(),
                            'data': analysis
                        }
                        self._save_cache()
                    
                    logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω")
                    return analysis
                else:
                    logger.warning("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞")
                    return self._create_enhanced_fallback(news_text, mentioned_tickers)
            
            elif response.status_code == 429:
                logger.warning("‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API")
                time.sleep(2)  # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                return self._analyze_with_retry(news_text, mentioned_tickers)
            
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API: {response.status_code}")
                logger.error(f"–¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏: {response.text[:200]}")
                return self._create_enhanced_fallback(news_text, mentioned_tickers)
                
        except requests.exceptions.Timeout:
            logger.error("‚åõÔ∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Yandex¬†GPT")
            return self._create_enhanced_fallback(news_text, mentioned_tickers)
            
        except requests.exceptions.ConnectionError:
            logger.error("üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Yandex¬†GPT")
            return self._create_enhanced_fallback(news_text, mentioned_tickers)
            
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
            return self._create_enhanced_fallback(news_text, mentioned_tickers)
    
    def _analyze_with_retry(self, news_text: str, mentioned_tickers: List[str], 
                           retries: int = 2) -> Dict[str, Any]:
        """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        for attempt in range(retries):
            logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}...")
            time.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è backoff-–∑–∞–¥–µ—Ä–∂–∫–∞
            
            try:
                return self.analyze_news(news_text, mentioned_tickers, use_cache=False)
            except Exception as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        
        return self._create_enhanced_fallback(news_text, mentioned_tickers)
    
    def _extract_json_from_response(self, text: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown –∫–æ–¥)
            json_patterns = [
                r'```json\n(.*?)\n```',  # JSON –≤ –±–ª–æ–∫–µ –∫–æ–¥–∞
                r'```\n(.*?)\n```',      # –ë–ª–æ–∫ –∫–æ–¥–∞ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —è–∑—ã–∫–∞
                r'({.*})',               # –ü—Ä–æ—Å—Ç–æ JSON –æ–±—ä–µ–∫—Ç
            ]
            
            for pattern in json_patterns:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                    return json.loads(json_str)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤—Ä—É—á–Ω—É—é
            start = text.find('{')
            end = text.rfind('}')
            if start != -1 and end != -1 and end > start:
                json_str = text[start:end+1]
                return json.loads(json_str)
                
        except json.JSONDecodeError as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON: {e}")
        
        return None
    
    def _create_basic_fallback(self, news_text: str, mentioned_tickers: List[str]) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤—ã–π fallback –∞–Ω–∞–ª–∏–∑"""
        return {
            "summary": f"–ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞: {news_text[:100]}...",
            "sentiment": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
            "risk_level": "—Å—Ä–µ–¥–Ω–∏–π",
            "affected_assets": mentioned_tickers if mentioned_tickers else ["RUB"],
            "market_impact": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "short_term_action": "–°–ª–µ–¥–∏—Ç—å –∑–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å–∏—Ç—É–∞—Ü–∏–∏",
            "is_fallback": True
        }
    
    def _create_enhanced_fallback(self, news_text: str, mentioned_tickers: List[str]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π fallback –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Å—Ç–æ–π –ª–æ–≥–∏–∫–∏"""
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_lower = news_text.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        positive_words = ["—Ä–æ—Å—Ç", "–≤—ã—à–µ", "–ø—Ä–∏–±—ã–ª—å", "—É—Å–ø–µ—Ö", "—Ä–µ–∫–æ—Ä–¥", "–ø–æ–≤—ã—à–µ–Ω–∏–µ"]
        negative_words = ["–ø–∞–¥–µ–Ω–∏–µ", "–Ω–∏–∂–µ", "—É–±—ã—Ç–æ–∫", "–∫—Ä–∏–∑–∏—Å", "–æ–±–≤–∞–ª", "—Å–Ω–∏–∂–µ–Ω–∏–µ"]
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            sentiment = "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
            sentiment_score = 0.5
        elif negative_count > positive_count:
            sentiment = "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
            sentiment_score = -0.5
        else:
            sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
            sentiment_score = 0.0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
        risk_words = ["–≤–æ–π–Ω–∞", "—Å–∞–Ω–∫—Ü–∏–∏", "–∫—Ä–∏–∑–∏—Å", "–¥–µ—Ñ–æ–ª—Ç", "–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ"]
        risk_count = sum(1 for word in risk_words if word in text_lower)
        
        if risk_count >= 2:
            risk_level = "–≤—ã—Å–æ–∫–∏–π"
        elif risk_count == 1:
            risk_level = "—Å—Ä–µ–¥–Ω–∏–π"
        else:
            risk_level = "–Ω–∏–∑–∫–∏–π"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        if sentiment == "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π":
            action = "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π"
        elif sentiment == "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π":
            action = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å, –≤–æ–∑–º–æ–∂–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è"
        else:
            action = "–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏, —Å–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤–æ—Å—Ç—è–º–∏"
        
        return {
            "summary": f"–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {news_text[:120]}...",
            "detailed_analysis": f"–ù–∞–π–¥–µ–Ω–æ {positive_count} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ {negative_count} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤",
            "sentiment": sentiment,
            "sentiment_score": sentiment_score,
            "risk_level": risk_level,
            "risk_explanation": f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {risk_count} —Å–ª–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞",
            "affected_assets": mentioned_tickers if mentioned_tickers else ["RUB"],
            "market_impact": "–õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –∞–∫—Ç–∏–≤—ã",
            "short_term_forecast": f"–û–∂–∏–¥–∞–µ—Ç—Å—è {sentiment} –¥–∏–Ω–∞–º–∏–∫–∞",
            "recommended_actions": action,
            "key_risks": ["–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞"] if risk_level != "–Ω–∏–∑–∫–∏–π" else [],
            "opportunities": ["–¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑"],
            "confidence": 0.6,
            "analyzed_at": datetime.now().isoformat(),
            "is_fallback": True
        }
    
    def analyze_multiple_news(self, news_list: List[Dict[str, Any]], 
                             batch_size: int = 3) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π [{"text": "...", "tickers": [...]}, ...]
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        """
        results = []
        
        for i in range(0, len(news_list), batch_size):
            batch = news_list[i:i+batch_size]
            logger.info(f"üì¶ –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {i//batch_size + 1} –∏–∑ {len(news_list)//batch_size + 1}")
            
            for news in batch:
                try:
                    analysis = self.analyze_news(
                        news.get("text", ""),
                        news.get("tickers", [])
                    )
                    results.append({
                        "news": news,
                        "analysis": analysis
                    })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    results.append({
                        "news": news,
                        "analysis": self._create_enhanced_fallback(
                            news.get("text", ""),
                            news.get("tickers", [])
                        ),
                        "error": str(e)
                    })
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + batch_size < len(news_list):
                time.sleep(1)
        
        return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            "total_requests": self.request_count,
            "cache_size": len(self.cache),
            "cache_enabled": self.cache_enabled,
            "folder_id": self.folder_id[:10] + "..." if self.folder_id else None,
            "last_cache_save": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def clear_cache(self, older_than_days: int = None):
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞"""
        if older_than_days:
            cutoff = datetime.now() - timedelta(days=older_than_days)
            keys_to_remove = []
            for key, value in self.cache.items():
                if value['timestamp'] < cutoff:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                del self.cache[key]
            
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(keys_to_remove)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫–µ—à–∞")
        else:
            self.cache.clear()
            logger.info("–ö–µ—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")
        
        self._save_cache()
    
    def enable_cache(self, enable: bool = True):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.cache_enabled = enable
        logger.info(f"–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ {'–≤–∫–ª—é—á–µ–Ω–æ' if enable else '–≤—ã–∫–ª—é—á–µ–Ω–æ'}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
yandex_analyzer: Optional[YandexGPTAnalyzer] = None

def init_analyzer(api_key: str, folder_id: str, cloud_id: str,
                  base_url: str = "https://llm.api.cloud.yandex.ru") -> bool:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ Yandex¬†GPT
    
    Args:
        api_key: 
        folder_id: 
        base_url: https://llm.api.cloud.yandex.ru
        cloud_id: 
    Returns:
        True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, –∏–Ω–∞—á–µ False
    """
    global yandex_analyzer
    
    try:
        if not api_key or not folder_id:
            logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã API –∫–ª—é—á –∏–ª–∏ folder ID")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç API –∫–ª—é—á–∞
        if len(api_key) < 20:
            logger.warning("‚ö†Ô∏è  API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º")
        
        yandex_analyzer = YandexGPTAnalyzer(api_key, folder_id, base_url)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        logger.info("üß™ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
        test_result = yandex_analyzer.analyze_news(
            "–¢–µ—Å—Ç–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å: –†—ã–Ω–æ–∫ –∞–∫—Ü–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å.",
            ["SBER", "GAZP"]
        )
        
        if test_result and "summary" in test_result:
            logger.info("‚úÖ Yandex¬†GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            return True
        else:
            logger.warning("‚ö†Ô∏è  –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Yandex¬†GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return False

def get_analyzer() -> Optional[YandexGPTAnalyzer]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    return yandex_analyzer

def analyze_text(text: str, tickers: List[str] = None) -> Dict[str, Any]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        tickers: –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
    """
    global yandex_analyzer
    
    if yandex_analyzer is None:
        logger.error("‚ùå –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return {
            "summary": "–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
            "error": "Yandex¬†GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω",
            "is_fallback": True
        }
    
    return yandex_analyzer.analyze_news(text, tickers or [])

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    test_api_key = "AQVNwXvLRG440CrVNnyttRBXbDn_5CeH0m-LBdBR"
    test_folder_id = "aje1ff5k8rhoq0ldadjs"
    test_cloud_id = "b1g1d5jm8n4ned90d6le"
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Yandex¬†GPT –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    if init_analyzer(test_api_key, test_folder_id, test_folder_id ):
        analyzer = get_analyzer()
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        test_news = """
        –ê–∫—Ü–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–æ—Å—Ç –Ω–∞ 2.5% –ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏. 
        –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –±–∞–Ω–∫–∞ –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –æ–∂–∏–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤ –Ω–∞ 15%. 
        –≠–∫—Å–ø–µ—Ä—Ç—ã –æ–∂–∏–¥–∞—é—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–æ—Å—Ç–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –Ω–∞ —Ñ–æ–Ω–µ —É–ª—É—á—à–µ–Ω–∏—è –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.
        """
        
        result = analyzer.analyze_news(test_news, ["SBER", "VTBR"])
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = analyzer.get_statistics()
        print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
    from config import YANDEX_API_KEY, YANDEX_FOLDER_ID, CLOUD_ID

if __name__ == "__main__":
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–≤–æ–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏
    if init_analyzer(YANDEX_API_KEY, YANDEX_FOLDER_ID, CLOUD_ID):
        print("‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –¢–í–û–ò–ú–ò –∫–ª—é—á–∞–º–∏")
        
        # –¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç—å—é
        test_news = "–ù–µ—Ñ—Ç—å Brent –≤—ã—Ä–æ—Å–ª–∞ –¥–æ $85 –ø–æ—Å–ª–µ –∞—Ç–∞–∫ –≤ –û—Ä–º—É–∑—Å–∫–æ–º –ø—Ä–æ–ª–∏–≤–µ"
        result = analyze_text(test_news, ["BRENT", "ROSN", "GAZP"])
        print(json.dumps(result, ensure_ascii=False, indent=2))